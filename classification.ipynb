{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMS ch 1</th>\n",
       "      <th>RMS ch 2</th>\n",
       "      <th>RMS ch 3</th>\n",
       "      <th>RMS ch 4</th>\n",
       "      <th>WL ch 1</th>\n",
       "      <th>WL ch 2</th>\n",
       "      <th>WL ch 3</th>\n",
       "      <th>WL ch 4</th>\n",
       "      <th>VAR ch 1</th>\n",
       "      <th>VAR ch 2</th>\n",
       "      <th>...</th>\n",
       "      <th>IEMG ch 3</th>\n",
       "      <th>IEMG ch 4</th>\n",
       "      <th>MF ch 1</th>\n",
       "      <th>MF ch 2</th>\n",
       "      <th>MF ch 3</th>\n",
       "      <th>MF ch 4</th>\n",
       "      <th>PF ch 1</th>\n",
       "      <th>PF ch 2</th>\n",
       "      <th>PF ch 3</th>\n",
       "      <th>PF ch 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24169.638117</td>\n",
       "      <td>22037.556050</td>\n",
       "      <td>25884.100462</td>\n",
       "      <td>25403.044523</td>\n",
       "      <td>11971.246999</td>\n",
       "      <td>11733.929519</td>\n",
       "      <td>11439.410309</td>\n",
       "      <td>10594.762967</td>\n",
       "      <td>5.830899e+08</td>\n",
       "      <td>4.846894e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>2.166425e+07</td>\n",
       "      <td>2.108159e+07</td>\n",
       "      <td>0.140810</td>\n",
       "      <td>0.159632</td>\n",
       "      <td>0.139763</td>\n",
       "      <td>0.127955</td>\n",
       "      <td>0.095000</td>\n",
       "      <td>0.164286</td>\n",
       "      <td>0.132143</td>\n",
       "      <td>0.065714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30740.508525</td>\n",
       "      <td>31338.339081</td>\n",
       "      <td>36954.679040</td>\n",
       "      <td>37596.902844</td>\n",
       "      <td>11429.241350</td>\n",
       "      <td>14215.002080</td>\n",
       "      <td>14178.488297</td>\n",
       "      <td>13637.256603</td>\n",
       "      <td>9.431838e+08</td>\n",
       "      <td>9.802175e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>2.905037e+07</td>\n",
       "      <td>3.089866e+07</td>\n",
       "      <td>0.130094</td>\n",
       "      <td>0.149665</td>\n",
       "      <td>0.132780</td>\n",
       "      <td>0.118098</td>\n",
       "      <td>0.072143</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.082143</td>\n",
       "      <td>0.063571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8388.862398</td>\n",
       "      <td>8155.628794</td>\n",
       "      <td>6360.908830</td>\n",
       "      <td>7997.637403</td>\n",
       "      <td>2432.665190</td>\n",
       "      <td>2471.937301</td>\n",
       "      <td>1882.118026</td>\n",
       "      <td>2077.414396</td>\n",
       "      <td>6.868396e+07</td>\n",
       "      <td>6.484023e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>6.441932e+06</td>\n",
       "      <td>6.455729e+06</td>\n",
       "      <td>0.105848</td>\n",
       "      <td>0.105093</td>\n",
       "      <td>0.107039</td>\n",
       "      <td>0.093530</td>\n",
       "      <td>0.047143</td>\n",
       "      <td>0.060714</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.087857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8259.850037</td>\n",
       "      <td>4376.061549</td>\n",
       "      <td>5167.477943</td>\n",
       "      <td>3403.009495</td>\n",
       "      <td>2620.078040</td>\n",
       "      <td>1523.315887</td>\n",
       "      <td>1598.337858</td>\n",
       "      <td>1041.545972</td>\n",
       "      <td>6.670489e+07</td>\n",
       "      <td>1.757596e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>4.729522e+06</td>\n",
       "      <td>3.355322e+06</td>\n",
       "      <td>0.107786</td>\n",
       "      <td>0.110372</td>\n",
       "      <td>0.107439</td>\n",
       "      <td>0.102975</td>\n",
       "      <td>0.054286</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.054286</td>\n",
       "      <td>0.085714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20474.603924</td>\n",
       "      <td>22001.149236</td>\n",
       "      <td>23427.753498</td>\n",
       "      <td>26926.624656</td>\n",
       "      <td>7689.225030</td>\n",
       "      <td>9459.301199</td>\n",
       "      <td>9066.090785</td>\n",
       "      <td>9179.519447</td>\n",
       "      <td>4.174177e+08</td>\n",
       "      <td>4.822388e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>1.818378e+07</td>\n",
       "      <td>2.197062e+07</td>\n",
       "      <td>0.126024</td>\n",
       "      <td>0.132797</td>\n",
       "      <td>0.126780</td>\n",
       "      <td>0.110890</td>\n",
       "      <td>0.090714</td>\n",
       "      <td>0.108571</td>\n",
       "      <td>0.082143</td>\n",
       "      <td>0.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>4844.845628</td>\n",
       "      <td>2876.859936</td>\n",
       "      <td>2846.516767</td>\n",
       "      <td>2410.784475</td>\n",
       "      <td>1657.756404</td>\n",
       "      <td>974.895517</td>\n",
       "      <td>889.368754</td>\n",
       "      <td>723.210753</td>\n",
       "      <td>2.183250e+07</td>\n",
       "      <td>6.663307e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.929320e+06</td>\n",
       "      <td>2.578630e+06</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>0.111446</td>\n",
       "      <td>0.115614</td>\n",
       "      <td>0.111373</td>\n",
       "      <td>0.095714</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.095714</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>8290.508086</td>\n",
       "      <td>4060.743937</td>\n",
       "      <td>4968.819015</td>\n",
       "      <td>3766.423562</td>\n",
       "      <td>3202.173413</td>\n",
       "      <td>1629.570409</td>\n",
       "      <td>1914.426536</td>\n",
       "      <td>1260.299221</td>\n",
       "      <td>6.694661e+07</td>\n",
       "      <td>1.479939e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>4.899197e+06</td>\n",
       "      <td>3.657839e+06</td>\n",
       "      <td>0.115805</td>\n",
       "      <td>0.118304</td>\n",
       "      <td>0.114317</td>\n",
       "      <td>0.105581</td>\n",
       "      <td>0.080714</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>0.080714</td>\n",
       "      <td>0.080714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>5152.306997</td>\n",
       "      <td>3042.006946</td>\n",
       "      <td>3307.939957</td>\n",
       "      <td>3306.681293</td>\n",
       "      <td>1655.653024</td>\n",
       "      <td>1069.027657</td>\n",
       "      <td>960.487822</td>\n",
       "      <td>986.741761</td>\n",
       "      <td>2.490306e+07</td>\n",
       "      <td>7.605079e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>3.170827e+06</td>\n",
       "      <td>3.350548e+06</td>\n",
       "      <td>0.116171</td>\n",
       "      <td>0.118251</td>\n",
       "      <td>0.112376</td>\n",
       "      <td>0.110729</td>\n",
       "      <td>0.041429</td>\n",
       "      <td>0.109286</td>\n",
       "      <td>0.108571</td>\n",
       "      <td>0.041429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>6158.476340</td>\n",
       "      <td>3426.773116</td>\n",
       "      <td>3899.959554</td>\n",
       "      <td>3302.184755</td>\n",
       "      <td>2137.551887</td>\n",
       "      <td>1259.485784</td>\n",
       "      <td>1298.172333</td>\n",
       "      <td>980.609955</td>\n",
       "      <td>3.630286e+07</td>\n",
       "      <td>1.010427e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>3.777414e+06</td>\n",
       "      <td>3.187580e+06</td>\n",
       "      <td>0.114029</td>\n",
       "      <td>0.120840</td>\n",
       "      <td>0.109991</td>\n",
       "      <td>0.102743</td>\n",
       "      <td>0.088571</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.088571</td>\n",
       "      <td>0.042857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>6114.830671</td>\n",
       "      <td>3960.993342</td>\n",
       "      <td>3510.877125</td>\n",
       "      <td>3664.616438</td>\n",
       "      <td>2108.297449</td>\n",
       "      <td>1442.653143</td>\n",
       "      <td>1088.214710</td>\n",
       "      <td>1013.294241</td>\n",
       "      <td>3.559244e+07</td>\n",
       "      <td>1.385977e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>3.360375e+06</td>\n",
       "      <td>3.295828e+06</td>\n",
       "      <td>0.119172</td>\n",
       "      <td>0.125474</td>\n",
       "      <td>0.108076</td>\n",
       "      <td>0.099158</td>\n",
       "      <td>0.087857</td>\n",
       "      <td>0.087857</td>\n",
       "      <td>0.081429</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMS ch 1      RMS ch 2      RMS ch 3      RMS ch 4       WL ch 1  \\\n",
       "0   24169.638117  22037.556050  25884.100462  25403.044523  11971.246999   \n",
       "1   30740.508525  31338.339081  36954.679040  37596.902844  11429.241350   \n",
       "2    8388.862398   8155.628794   6360.908830   7997.637403   2432.665190   \n",
       "3    8259.850037   4376.061549   5167.477943   3403.009495   2620.078040   \n",
       "4   20474.603924  22001.149236  23427.753498  26926.624656   7689.225030   \n",
       "..           ...           ...           ...           ...           ...   \n",
       "88   4844.845628   2876.859936   2846.516767   2410.784475   1657.756404   \n",
       "89   8290.508086   4060.743937   4968.819015   3766.423562   3202.173413   \n",
       "90   5152.306997   3042.006946   3307.939957   3306.681293   1655.653024   \n",
       "91   6158.476340   3426.773116   3899.959554   3302.184755   2137.551887   \n",
       "92   6114.830671   3960.993342   3510.877125   3664.616438   2108.297449   \n",
       "\n",
       "         WL ch 2       WL ch 3       WL ch 4      VAR ch 1      VAR ch 2  ...  \\\n",
       "0   11733.929519  11439.410309  10594.762967  5.830899e+08  4.846894e+08  ...   \n",
       "1   14215.002080  14178.488297  13637.256603  9.431838e+08  9.802175e+08  ...   \n",
       "2    2471.937301   1882.118026   2077.414396  6.868396e+07  6.484023e+07  ...   \n",
       "3    1523.315887   1598.337858   1041.545972  6.670489e+07  1.757596e+07  ...   \n",
       "4    9459.301199   9066.090785   9179.519447  4.174177e+08  4.822388e+08  ...   \n",
       "..           ...           ...           ...           ...           ...  ...   \n",
       "88    974.895517    889.368754    723.210753  2.183250e+07  6.663307e+06  ...   \n",
       "89   1629.570409   1914.426536   1260.299221  6.694661e+07  1.479939e+07  ...   \n",
       "90   1069.027657    960.487822    986.741761  2.490306e+07  7.605079e+06  ...   \n",
       "91   1259.485784   1298.172333    980.609955  3.630286e+07  1.010427e+07  ...   \n",
       "92   1442.653143   1088.214710   1013.294241  3.559244e+07  1.385977e+07  ...   \n",
       "\n",
       "       IEMG ch 3     IEMG ch 4   MF ch 1   MF ch 2   MF ch 3   MF ch 4  \\\n",
       "0   2.166425e+07  2.108159e+07  0.140810  0.159632  0.139763  0.127955   \n",
       "1   2.905037e+07  3.089866e+07  0.130094  0.149665  0.132780  0.118098   \n",
       "2   6.441932e+06  6.455729e+06  0.105848  0.105093  0.107039  0.093530   \n",
       "3   4.729522e+06  3.355322e+06  0.107786  0.110372  0.107439  0.102975   \n",
       "4   1.818378e+07  2.197062e+07  0.126024  0.132797  0.126780  0.110890   \n",
       "..           ...           ...       ...       ...       ...       ...   \n",
       "88  2.929320e+06  2.578630e+06  0.114583  0.111446  0.115614  0.111373   \n",
       "89  4.899197e+06  3.657839e+06  0.115805  0.118304  0.114317  0.105581   \n",
       "90  3.170827e+06  3.350548e+06  0.116171  0.118251  0.112376  0.110729   \n",
       "91  3.777414e+06  3.187580e+06  0.114029  0.120840  0.109991  0.102743   \n",
       "92  3.360375e+06  3.295828e+06  0.119172  0.125474  0.108076  0.099158   \n",
       "\n",
       "     PF ch 1   PF ch 2   PF ch 3   PF ch 4  \n",
       "0   0.095000  0.164286  0.132143  0.065714  \n",
       "1   0.072143  0.085000  0.082143  0.063571  \n",
       "2   0.047143  0.060714  0.060000  0.087857  \n",
       "3   0.054286  0.060000  0.054286  0.085714  \n",
       "4   0.090714  0.108571  0.082143  0.070000  \n",
       "..       ...       ...       ...       ...  \n",
       "88  0.095714  0.060000  0.095714  0.005000  \n",
       "89  0.080714  0.089286  0.080714  0.080714  \n",
       "90  0.041429  0.109286  0.108571  0.041429  \n",
       "91  0.088571  0.060000  0.088571  0.042857  \n",
       "92  0.087857  0.087857  0.081429  0.071429  \n",
       "\n",
       "[93 rows x 24 columns]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from mat file\n",
    "mat = scipy.io.loadmat(\"exampleEMGdata180trial_train.mat\")\n",
    "\n",
    "# load feature data from csv\n",
    "features = pd.read_csv('features.csv')\n",
    "# test_features = pd.read_csv('test_features.csv')\n",
    "test_features = pd.read_csv('feature_table.csv')\n",
    "\n",
    "# make individual label tensor / column\n",
    "labels = features.pop('labels')\n",
    "test_labels = test_features.pop('labels')\n",
    "\n",
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make rms features\n",
    "rms_headers = [f\"RMS ch {num}\" for num in [x+1 for x in range(4)]]\n",
    "rms_features = features[rms_headers]\n",
    "\n",
    "# make wl features\n",
    "wl_headers = [f\"WL ch {num}\" for num in [x+1 for x in range(4)]]\n",
    "wl_features = features[wl_headers]\n",
    "\n",
    "# make var features\n",
    "var_headers = [f\"VAR ch {num}\" for num in [x+1 for x in range(4)]]\n",
    "var_features = features[var_headers]\n",
    "\n",
    "# make iemg features\n",
    "iemg_headers = [f\"IEMG ch {num}\" for num in [x+1 for x in range(4)]]\n",
    "iemg_features = features[iemg_headers]\n",
    "\n",
    "# make mf features\n",
    "mf_headers = [f\"MF ch {num}\" for num in [x+1 for x in range(4)]]\n",
    "mf_features = features[mf_headers]\n",
    "\n",
    "# make pf features\n",
    "pf_headers = [f\"PF ch {num}\" for num in [x+1 for x in range(4)]]\n",
    "pf_features = features[pf_headers]\n",
    "\n",
    "# make best feature\n",
    "best_feature = pd.concat([mf_features, pf_features], axis=1, join='inner')\n",
    "\n",
    "# make mf test features\n",
    "test_mf_features = test_features[mf_headers]\n",
    "\n",
    "# make pf test features\n",
    "test_pf_features = test_features[pf_headers]\n",
    "\n",
    "# make best test feature\n",
    "test_best_feature = pd.concat([test_mf_features, test_pf_features], axis=1, join='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(feature, label):\n",
    "\n",
    "#     METRICS = [\n",
    "#       tf.keras.metrics.CategoricalAccuracy(name='accuracy')\n",
    "# ]\n",
    "\n",
    "    normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "\n",
    "    normalizer.adapt(feature)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        normalizer,\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(2187, activation='relu'),\n",
    "        tf.keras.layers.Dense(729, activation='relu'),\n",
    "        tf.keras.layers.Dense(243, activation='relu'),\n",
    "        tf.keras.layers.Dense(81, activation='relu'),\n",
    "        tf.keras.layers.Dense(27, activation='relu'),\n",
    "        tf.keras.layers.Dense(3, activation='softmax'),\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=tf.keras.metrics.CategoricalAccuracy(name='accuracy'))\n",
    "\n",
    "    model.fit(feature, label, epochs=50)\n",
    "\n",
    "    loss, acc = model.evaluate(feature, label)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rms_acc = get_accuracy(rms_features, labels)\n",
    "# wl_acc = get_accuracy(wl_features, labels)\n",
    "# var_acc = get_accuracy(var_features, labels)\n",
    "# iemg_acc = get_accuracy(iemg_features, labels)\n",
    "# mf_acc = get_accuracy(mf_features, labels)\n",
    "# pf_acc = get_accuracy (pf_features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracies = [rms_acc, wl_acc, var_acc, iemg_acc, mf_acc, pf_acc]\n",
    "\n",
    "# x = 1\n",
    "# for acc in accuracies:\n",
    "#     plt.bar(x, acc, width=0.5)\n",
    "#     x += 1\n",
    "\n",
    "# plt.xlabel('Feature Type')\n",
    "# plt.ylabel('Training Accuracy')\n",
    "# plt.title('Resubstitution Accuracy of RPS Training Data Features')\n",
    "# plt.legend(['Root Mean Square', 'Waveform Length', 'Variance', 'Integrated EMG', 'Mean Frequency', 'Peak Frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_best_acc, train_tp, train_fp, train_tn, train_fn, train_pre, train_re= get_accuracy(best_feature, labels)\n",
    "# test_best_acc, test_tp, test_fp, test_tn, test_fn, test_pre, test_re = get_accuracy(test_best_feature, test_labels)\n",
    "# train_results = get_accuracy(best_feature, labels)\n",
    "# test_results = get_accuracy(test_best_feature, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mampane\\OneDrive - Olin College of Engineering\\Desktop\\Olin\\Second Year First Semester\\Neurotech\\Analysis of Pre-Collected EMG Data\\.venv\\Lib\\site-packages\\keras\\src\\backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 12ms/step - loss: 1.1173 - accuracy: 0.3056\n",
      "Epoch 2/20\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1.0697 - accuracy: 0.4000\n",
      "Epoch 3/20\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1.0654 - accuracy: 0.4111\n",
      "Epoch 4/20\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.0178 - accuracy: 0.4556\n",
      "Epoch 5/20\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.9928 - accuracy: 0.4667\n",
      "Epoch 6/20\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.9690 - accuracy: 0.4556\n",
      "Epoch 7/20\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.9358 - accuracy: 0.5222\n",
      "Epoch 8/20\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.9208 - accuracy: 0.5333\n",
      "Epoch 9/20\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.8857 - accuracy: 0.5944\n",
      "Epoch 10/20\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.8317 - accuracy: 0.6222\n",
      "Epoch 11/20\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.8095 - accuracy: 0.6389\n",
      "Epoch 12/20\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.7842 - accuracy: 0.6611\n",
      "Epoch 13/20\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.7626 - accuracy: 0.6444\n",
      "Epoch 14/20\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.7128 - accuracy: 0.7056\n",
      "Epoch 15/20\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6851 - accuracy: 0.7111\n",
      "Epoch 16/20\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6537 - accuracy: 0.7389\n",
      "Epoch 17/20\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6310 - accuracy: 0.7444\n",
      "Epoch 18/20\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6373 - accuracy: 0.7111\n",
      "Epoch 19/20\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.5845 - accuracy: 0.7500\n",
      "Epoch 20/20\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.5780 - accuracy: 0.7500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 2.2053 - accuracy: 0.3118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mampane\\OneDrive - Olin College of Engineering\\Desktop\\Olin\\Second Year First Semester\\Neurotech\\Analysis of Pre-Collected EMG Data\\.venv\\Lib\\site-packages\\keras\\src\\backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    }
   ],
   "source": [
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "\n",
    "normalizer.adapt(best_feature)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(2187, activation='relu'),\n",
    "    tf.keras.layers.Dense(729, activation='relu'),\n",
    "    tf.keras.layers.Dense(243, activation='relu'),\n",
    "    tf.keras.layers.Dense(81, activation='relu'),\n",
    "    tf.keras.layers.Dense(27, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            # metrics=tf.keras.metrics.CategoricalAccuracy(name='accuracy')\n",
    "            metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "model.fit(best_feature, labels, epochs=20)\n",
    "\n",
    "loss, acc = model.evaluate(test_best_feature, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction_vector = model.predict(test_best_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(prediction_vector[0]).index(max(prediction_vector[0]))\n",
    "# list(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = [list(prediction_vector[idx]).index(max(list(prediction_vector)[idx])) for idx in range(len(prediction_vector)) if list(prediction_vector[idx]).index(max(list(prediction_vector)[idx])) == test_labels[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3118279569892473"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_predictions)/len(prediction_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
